#!/usr/bin/env python
# encoding: utf-8
"""
XHS Graph Data Processor V2 - FIXED VERSION
Solves Feature Representation Gap and Emoji Vocabulary Limitation

Key Improvements:
1. Uses actual vocabularies from the XHS database
2. Implements TF-IDF features for words (matching training data)
3. Uses proper emoji vocabulary from database
4. Creates features compatible with pre-trained model
5. Leverages learned embeddings correctly
"""

import torch
import dgl
import numpy as np
import re
import jieba
import sqlite3
import json
from typing import List, Dict, Tuple, Set, Optional
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import logging

logger = logging.getLogger(__name__)

class XHSGraphProcessorV2:
    """Enhanced processor that uses real XHS data vocabularies and proper features"""
    
    def __init__(self, db_path: str = "xhs_data.db", load_vocabularies: bool = True):
        """
        Initialize with real XHS database vocabularies
        
        Args:
            db_path: Path to XHS database
            load_vocabularies: Whether to load vocabularies from database
        """
        
        # Emoji pattern for extraction
        self.emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "\U0001F1E0-\U0001F1FF"  # flags (iOS)
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "]+", flags=re.UNICODE
        )
        
        # Initialize vocabularies
        self.word_vocab = {}
        self.emoji_vocab = {}
        self.post_vocab = {}
        self.word_idf_scores = {}
        
        # TF-IDF vectorizer for words
        self.tfidf_vectorizer = None
        self.word_tfidf_matrix = None
        
        # Database connection
        self.db_path = db_path
        
        # Load real vocabularies from database
        if load_vocabularies:
            self._load_vocabularies_from_database()
    
    def _load_vocabularies_from_database(self):
        """Load actual vocabularies from XHS database"""
        logger.info("Loading vocabularies from XHS database...")
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # 1. Load word vocabulary with IDF scores
            logger.info("Loading word vocabulary with IDF scores...")
            cursor.execute("SELECT word, idf FROM word_idf ORDER BY idf DESC LIMIT 10000")
            word_idf_data = cursor.fetchall()
            
            for i, (word, idf) in enumerate(word_idf_data):
                self.word_vocab[word] = i
                self.word_idf_scores[word] = idf
            
            logger.info(f"Loaded {len(self.word_vocab)} words with IDF scores")
            
            # 2. Extract emoji vocabulary from actual posts
            logger.info("Extracting emoji vocabulary from posts...")
            cursor.execute("""
                SELECT title, content FROM note_info 
                WHERE title IS NOT NULL AND content IS NOT NULL 
                LIMIT 50000
            """)
            
            emoji_counter = Counter()
            post_count = 0
            
            for title, content in cursor.fetchall():
                if not title or not content:
                    continue
                    
                full_text = f"{title} {content}"
                emojis = self.extract_emojis(full_text)
                emoji_counter.update(emojis)
                post_count += 1
                
                if post_count % 10000 == 0:
                    logger.info(f"Processed {post_count} posts for emoji extraction")
            
            # Build emoji vocabulary from most common emojis
            for i, (emoji, count) in enumerate(emoji_counter.most_common(5000)):
                self.emoji_vocab[emoji] = i
            
            logger.info(f"Extracted {len(self.emoji_vocab)} emojis from {post_count} posts")
            
            # 3. Prepare TF-IDF vectorizer with real vocabulary
            logger.info("Preparing TF-IDF vectorizer...")
            self._setup_tfidf_vectorizer()
            
            conn.close()
            
            # Save vocabularies for future use
            self._save_vocabularies()
            
            logger.info("âœ… Successfully loaded all vocabularies from database")
            
        except Exception as e:
            logger.error(f"Error loading vocabularies from database: {e}")
            logger.info("Falling back to default vocabularies...")
            self._create_default_vocabularies()
    
    def _setup_tfidf_vectorizer(self):
        """Setup TF-IDF vectorizer with database vocabulary"""
        # Use the words from database as vocabulary
        vocabulary = list(self.word_vocab.keys())
        
        # Create TF-IDF vectorizer with database vocabulary
        self.tfidf_vectorizer = TfidfVectorizer(
            vocabulary=vocabulary,
            lowercase=False,
            token_pattern=None,
            tokenizer=self._tokenize_chinese,
            max_features=len(vocabulary)
        )
        
        # Fit on a dummy corpus to initialize
        dummy_corpus = [" ".join(vocabulary[:100])]  # Use first 100 words
        self.tfidf_vectorizer.fit(dummy_corpus)
        
        logger.info(f"TF-IDF vectorizer initialized with {len(vocabulary)} words")
    
    def _tokenize_chinese(self, text: str) -> List[str]:
        """Tokenize Chinese text using jieba"""
        # Remove emojis first
        clean_text = self.emoji_pattern.sub('', text)
        # Tokenize
        words = list(jieba.cut(clean_text.strip()))
        # Filter and return words that are in vocabulary
        return [w for w in words if w in self.word_vocab]
    
    def _create_default_vocabularies(self):
        """Create default vocabularies as fallback"""
        logger.info("Creating default vocabularies...")
        
        # Default high-frequency Chinese words
        default_words = [
            "çš„", "ä¸€", "æ˜¯", "åœ¨", "ä¸", "äº†", "æœ‰", "å’Œ", "äºº", "è¿™",
            "ä¸­", "å¤§", "ä¸º", "ä¸Š", "ä¸ª", "å›½", "æˆ‘", "ä»¥", "è¦", "ä»–",
            "æ—¶", "æ¥", "ç”¨", "ä»¬", "ç”Ÿ", "åˆ°", "ä½œ", "åœ°", "äºŽ", "å‡º",
            "å°±", "åˆ†", "å¯¹", "æˆ", "ä¼š", "å¯", "ä¸»", "å‘", "å¹´", "åŠ¨",
            "åŒ", "å·¥", "ä¹Ÿ", "èƒ½", "ä¸‹", "è¿‡", "å­", "è¯´", "äº§", "ç§",
            "é¢", "è€Œ", "æ–¹", "åŽ", "å¤š", "å®š", "è¡Œ", "å­¦", "æ³•", "æ‰€",
            "æ°‘", "å¾—", "ç»", "å", "ä¸‰", "ä¹‹", "è¿›", "ç€", "ç­‰", "éƒ¨",
            "åº¦", "å®¶", "ç”µ", "åŠ›", "é‡Œ", "å¦‚", "æ°´", "åŒ–", "é«˜", "è‡ª",
            "äºŒ", "ç†", "èµ·", "å°", "ç‰©", "çŽ°", "å®ž", "åŠ ", "é‡", "éƒ½",
            "ä¸¤", "ä½“", "åˆ¶", "æœº", "å½“", "ä½¿", "ç‚¹", "ä»Ž", "ä¸š", "æœ¬",
            "åŽ»", "æŠŠ", "æ€§", "å¥½", "åº”", "å¼€", "å®ƒ", "åˆ", "è¿˜", "å› ",
            "ç”±", "å…¶", "äº›", "ç„¶", "å‰", "å¤–", "å¤©", "æ”¿", "å››", "æ—¥",
            "é‚£", "ç¤¾", "ä¹‰", "äº‹", "å¹³", "å½¢", "ç›¸", "å…¨", "è¡¨", "é—´",
            "æ ·", "ä¸Ž", "å…³", "å„", "é‡", "æ–°", "çº¿", "å†…", "æ•°", "æ­£",
            "å¿ƒ", "å", "ä½ ", "æ˜Ž", "çœ‹", "åŽŸ", "åˆ", "ä¹ˆ", "åˆ©", "æ¯”",
            "æˆ–", "ä½†", "è´¨", "æ°”", "ç¬¬", "å‘", "é“", "å‘½", "æ­¤", "å˜",
            "æ¡", "åª", "æ²¡", "ç»“", "è§£", "é—®", "æ„", "å»º", "æœˆ", "å…¬",
            "æ— ", "ç³»", "å†›", "å¾ˆ", "æƒ…", "è€…", "æœ€", "ç«‹", "ä»£", "æƒ³",
            "å·²", "é€š", "å¹¶", "æ", "ç›´", "é¢˜", "å…š", "ç¨‹", "å±•", "äº”",
            "æžœ", "æ–™", "è±¡", "å‘˜", "é©", "ä½", "å…¥", "å¸¸", "æ–‡", "æ€»",
            "æ¬¡", "å“", "å¼", "æ´»", "è®¾", "åŠ", "ç®¡", "ç‰¹", "ä»¶", "é•¿",
            "æ±‚", "è€", "å¤´", "åŸº", "èµ„", "è¾¹", "æµ", "è·¯", "çº§", "å°‘",
            "å›¾", "å±±", "ç»Ÿ", "æŽ¥", "çŸ¥", "è¾ƒ", "å°†", "ç»„", "è§", "è®¡",
            "åˆ«", "å¥¹", "æ‰‹", "è§’", "æœŸ", "æ ¹", "è®º", "è¿", "å†œ", "æŒ‡",
            "å‡ ", "ä¹", "åŒº", "å¼º", "æ”¾", "å†³", "è¥¿", "è¢«", "å¹²", "åš",
            "å¿…", "æˆ˜", "å…ˆ", "å›ž", "åˆ™", "ä»»", "å–", "æ®", "å¤„", "é˜Ÿ",
            "å—", "ç»™", "è‰²", "å…‰", "é—¨", "å³", "ä¿", "æ²»", "åŒ—", "é€ ",
            "ç™¾", "è§„", "çƒ­", "é¢†", "ä¸ƒ", "æµ·", "å£", "ä¸œ", "å¯¼", "å™¨",
            "åŽ‹", "å¿—", "ä¸–", "é‡‘", "å¢ž", "äº‰", "æµŽ", "é˜¶", "æ²¹", "æ€",
            "æœ¯", "æž", "äº¤", "å—", "è”", "ä»€", "è®¤", "å…­", "å…±", "æƒ",
            "æ”¶", "è¯", "æ”¹", "æ¸…", "ç¾Ž", "å†", "é‡‡", "è½¬", "æ›´", "å•",
            "é£Ž", "åˆ‡", "æ‰“", "ç™½", "æ•™", "é€Ÿ", "èŠ±", "å¸¦", "å®‰", "åœº",
            "èº«", "è½¦", "ä¾‹", "çœŸ", "åŠ¡", "å…·", "ä¸‡", "æ¯", "ç›®", "è‡³",
            "è¾¾", "èµ°", "ç§¯", "ç¤º", "è®®", "å£°", "æŠ¥", "æ–—", "å®Œ", "ç±»",
            "å…«", "ç¦»", "åŽ", "å", "ç¡®", "æ‰", "ç§‘", "å¼ ", "ä¿¡", "é©¬",
            "èŠ‚", "è¯", "ç±³", "æ•´", "ç©º", "å…ƒ", "å†µ", "ä»Š", "é›†", "æ¸©",
            "ä¼ ", "åœŸ", "è®¸", "æ­¥", "ç¾¤", "å¹¿", "çŸ³", "è®°", "éœ€", "æ®µ",
            "ç ”", "ç•Œ", "æ‹‰", "æž—", "å¾‹", "å«", "ä¸”", "ç©¶", "è§‚", "è¶Š",
            "ç»‡", "è£…", "å½±", "ç®—", "ä½Ž", "æŒ", "éŸ³", "ä¼—", "ä¹¦", "å¸ƒ",
            "å¤", "å®¹", "å„¿", "é¡»", "é™…", "å•†", "éž", "éªŒ", "è¿ž", "æ–­",
            "æ·±", "éš¾", "è¿‘", "çŸ¿", "åƒ", "å‘¨", "å§”", "ç´ ", "æŠ€", "å¤‡",
            "åŠ", "åŠž", "é’", "çœ", "åˆ—", "ä¹ ", "å“", "çº¦", "æ”¯", "èˆ¬",
            "å²", "æ„Ÿ", "åŠ³", "ä¾¿", "å›¢", "å¾€", "é…¸", "åŽ†", "å¸‚", "å…‹",
            "ä½•", "é™¤", "æ¶ˆ", "æž„", "åºœ", "ç§°", "å¤ª", "å‡†", "ç²¾", "å€¼",
            "å·", "çŽ‡", "æ—", "ç»´", "åˆ’", "é€‰", "æ ‡", "å†™", "å­˜", "å€™",
            "æ¯›", "äº²", "å¿«", "æ•ˆ", "æ–¯", "é™¢", "æŸ¥", "æ±Ÿ", "åž‹", "çœ¼",
            "çŽ‹", "æŒ‰", "æ ¼", "å…»", "æ˜“", "ç½®", "æ´¾", "å±‚", "ç‰‡", "å§‹",
            "å´", "ä¸“", "çŠ¶", "è‚²", "åŽ‚", "äº¬", "è¯†", "é€‚", "å±ž", "åœ†",
            "åŒ…", "ç«", "ä½", "è°ƒ", "æ»¡", "åŽ¿", "å±€", "ç…§", "å‚", "çº¢",
            "ç»†", "å¼•", "å¬", "è¯¥", "é“", "ä»·", "ä¸¥", "é¦–", "åº•", "æ¶²",
            "å®˜", "å¾·", "éš", "ç—…", "è‹", "å¤±", "å°”", "æ­»", "è®²", "é…",
            "å¥³", "é»„", "æŽ¨", "æ˜¾", "è°ˆ", "ç½ª", "ç¥ž", "è‰º", "å‘¢", "å¸­",
            "å«", "ä¼", "æœ›", "å¯†", "æ‰¹", "è¥", "é¡¹", "é˜²", "ä¸¾", "çƒ",
            "è‹±", "æ°§", "åŠ¿", "å‘Š", "æŽ", "å°", "è½", "æœ¨", "å¸®", "è½®",
            "ç ´", "äºš", "å¸ˆ", "å›´", "æ³¨", "è¿œ", "å­—", "æ", "æŽ’", "ä¾›",
            "æ²³", "æ€", "å°", "å¦", "æ–½", "å‡", "æ ‘", "æº¶", "æ€Ž", "æ­¢",
            "æ¡ˆ", "è¨€", "å£«", "å‡", "æ­¦", "å›º", "å¶", "é±¼", "æ³¢", "è§†",
            "ä»…", "è´¹", "ç´§", "çˆ±", "å·¦", "ç« ", "æ—©", "æœ", "å®³", "ç»­",
            "è½»", "æœ", "è¯•", "é£Ÿ", "å……", "å…µ", "æº", "åˆ¤", "æŠ¤", "å¸",
            "è¶³", "æŸ", "ç»ƒ", "å·®", "è‡´", "æ¿", "ç”°", "é™", "é»‘", "çŠ¯",
            "è´Ÿ", "å‡»", "èŒƒ", "ç»§", "å…´", "ä¼¼", "ä½™", "åš", "æ›²", "è¾“",
            "ä¿®", "æ•…", "åŸŽ", "å¤«", "å¤Ÿ", "é€", "ç¬”", "èˆ¹", "å ", "å³",
            "è´¢", "åƒ", "å¯Œ", "æ˜¥", "èŒ", "è§‰", "æ±‰", "ç”»", "åŠŸ", "å·´",
            "è·Ÿ", "è™½", "æ‚", "é£ž", "æ£€", "å¸", "åŠ©", "å‡", "é˜³", "äº’",
            "åˆ", "åˆ›", "æŠ—", "è€ƒ", "æŠ•", "å", "ç­–", "å¤", "å¾„", "æ¢",
            "æœª", "è·‘", "ç•™", "é’¢", "æ›¾", "ç«¯", "è´£", "ç«™", "ç®€", "è¿°",
            "é’±", "å‰¯", "å°½", "å¸", "å°„", "è‰", "å†²", "æ‰¿", "ç‹¬", "ä»¤",
            "é™", "é˜¿", "å®£", "çŽ¯", "åŒ", "è¯·", "è¶…", "å¾®", "è®©", "æŽ§",
            "å·ž", "è‰¯", "è½´", "æ‰¾", "å¦", "çºª", "ç›Š", "ä¾", "ä¼˜", "é¡¶",
            "ç¡€", "è½½", "å€’", "æˆ¿", "çª", "å", "ç²‰", "æ•Œ", "ç•¥", "å®¢",
            "è¢", "å†·", "èƒœ", "ç»", "æž", "å—", "å‰‚", "æµ‹", "ä¸", "å",
            "è¯‰", "å¿µ", "é™ˆ", "ä»", "ç½—", "ç›", "å‹", "æ´‹", "é”™", "è‹¦",
            "å¤œ", "åˆ‘", "ç§»", "é¢‘", "é€", "é ", "æ··", "æ¯", "çŸ­", "çš®",
            "ç»ˆ", "èš", "æ±½", "æ‘", "äº‘", "å“ª", "æ—¢", "è·", "å«", "åœ",
            "çƒˆ", "å¤®", "å¯Ÿ", "çƒ§", "è¿…", "å¢ƒ", "è‹¥", "å°", "æ´²", "åˆ»",
            "æ‹¬", "æ¿€", "å­”", "æž", "ç”š", "å®¤", "å¾…", "æ ¸", "æ ¡", "æ•£",
            "ä¾µ", "å§", "ç”²", "æ¸¸", "ä¹…", "èœ", "å‘³", "æ—§", "æ¨¡", "æ¹–",
            "è´§", "æŸ", "é¢„", "é˜»", "æ¯«", "æ™®", "ç¨³", "ä¹™", "å¦ˆ", "æ¤",
            "æ¯", "æ‰©", "é“¶", "è¯­", "æŒ¥", "é…’", "å®ˆ", "æ‹¿", "åº", "çº¸",
            "åŒ»", "ç¼º", "é›¨", "å—", "é’ˆ", "åˆ˜", "å•Š", "æ€¥", "å”±", "è¯¯",
            "è®­", "æ„¿", "å®¡", "é™„", "èŽ·", "èŒ¶", "é²œ", "ç²®", "æ–¤", "å­©",
            "è„±", "ç¡«", "è‚¥", "å–„", "é¾™", "æ¼”", "çˆ¶", "æ¸", "è¡€", "æ¬¢",
            "æ¢°", "æŽŒ", "æ­Œ", "æ²™", "åˆš", "æ”»", "è°“", "ç›¾", "è®¨", "æ™š",
            "ç²’", "ä¹±", "ç‡ƒ", "çŸ›", "ä¹Ž", "æ€", "è¯", "å®", "é²", "è´µ",
            "é’Ÿ", "ç…¤", "è¯»", "ç­", "ä¼¯", "é¦™", "ä»‹", "è¿«", "å¥", "ä¸°",
            "åŸ¹", "æ¡", "å…°", "æ‹…", "å¼¦", "è›‹", "æ²‰", "å‡", "ç©¿", "æ‰§",
            "ç­”", "ä¹", "è°", "é¡º", "çƒŸ", "ç¼©", "å¾", "è„¸", "å–œ", "æ¾",
            "è„š", "å›°", "å¼‚", "å…", "èƒŒ", "æ˜Ÿ", "ç¦", "ä¹°", "æŸ“", "äº•",
            "æ¦‚", "æ…¢", "æ€•", "ç£", "å€", "ç¥–", "çš‡", "ä¿ƒ", "é™", "è¡¥",
            "è¯„", "ç¿»", "è‚‰", "è·µ", "å°¼", "è¡£", "å®½", "æ‰¬", "æ£‰", "å¸Œ",
            "ä¼¤", "æ“", "åž‚", "ç§‹", "å®œ", "æ°¢", "å¥—", "ç£", "æŒ¯", "æž¶",
            "äº®", "æœ«", "å®ª", "åº†", "ç¼–", "ç‰›", "è§¦", "æ˜ ", "é›·", "é”€",
            "è¯—", "åº§", "å±…", "æŠ“", "è£‚", "èƒž", "å‘¼", "å¨˜", "æ™¯", "å¨",
            "ç»¿", "æ™¶", "åŽš", "ç›Ÿ", "è¡¡", "é¸¡", "å­™", "å»¶", "å±", "èƒ¶",
            "å±‹", "ä¹¡", "ä¸´", "é™†", "é¡¾", "æŽ‰", "å‘€", "ç¯", "å²", "æŽª",
            "æŸ", "åˆ€", "æ¶", "åœ", "è‚²", "å±Š", "æ¬§", "çŒ®", "æ”¯", "è¾…"
        ]
        
        # Build word vocabulary
        for i, word in enumerate(default_words):
            self.word_vocab[word] = i
            self.word_idf_scores[word] = 1.0  # Default IDF score
        
        # NO DEFAULT EMOJIS - only use database vocabulary
        # This ensures we only work with real XHS emoji data
        logger.warning("No database connection available - emoji vocabulary will be empty!")
        logger.warning("This may affect emoji suggestion quality. Please check database connection.")
        
        # Setup basic TF-IDF
        self._setup_tfidf_vectorizer()
        
        logger.info(f"Created default vocabularies: {len(self.word_vocab)} words, {len(self.emoji_vocab)} emojis")
    
    def _save_vocabularies(self):
        """Save vocabularies to file for future use"""
        vocab_data = {
            'word_vocab': self.word_vocab,
            'emoji_vocab': self.emoji_vocab,
            'word_idf_scores': self.word_idf_scores
        }
        
        with open('xhs_vocabularies_v2.json', 'w', encoding='utf-8') as f:
            json.dump(vocab_data, f, ensure_ascii=False, indent=2)
        
        logger.info("Vocabularies saved to xhs_vocabularies_v2.json")
    
    def load_vocabularies_from_file(self, vocab_file: str = 'xhs_vocabularies_v2.json'):
        """Load vocabularies from saved file"""
        try:
            with open(vocab_file, 'r', encoding='utf-8') as f:
                vocab_data = json.load(f)
            
            self.word_vocab = vocab_data['word_vocab']
            self.emoji_vocab = vocab_data['emoji_vocab']
            self.word_idf_scores = vocab_data['word_idf_scores']
            
            self._setup_tfidf_vectorizer()
            
            logger.info(f"Loaded vocabularies from {vocab_file}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to load vocabularies from {vocab_file}: {e}")
            return False
    
    def extract_emojis(self, text: str) -> List[str]:
        """Extract emojis from text"""
        return self.emoji_pattern.findall(text)
    
    def extract_words(self, text: str) -> List[str]:
        """Extract words from Chinese text using jieba"""
        return self._tokenize_chinese(text)
    
    def create_tfidf_features(self, posts: List[str]) -> torch.Tensor:
        """
        Create TF-IDF features for posts using database vocabulary
        This matches the original training data format
        """
        try:
            # Transform posts to TF-IDF vectors
            tfidf_matrix = self.tfidf_vectorizer.transform(posts)
            
            # Convert to dense tensor
            tfidf_dense = tfidf_matrix.toarray()
            
            # Pad or truncate to 768 dimensions
            if tfidf_dense.shape[1] < 768:
                # Pad with zeros
                padding = np.zeros((tfidf_dense.shape[0], 768 - tfidf_dense.shape[1]))
                tfidf_dense = np.concatenate([tfidf_dense, padding], axis=1)
            elif tfidf_dense.shape[1] > 768:
                # Truncate to 768 dimensions
                tfidf_dense = tfidf_dense[:, :768]
            
            return torch.tensor(tfidf_dense, dtype=torch.float32)
            
        except Exception as e:
            logger.error(f"Error creating TF-IDF features: {e}")
            # Fallback to simple features
            return self._create_simple_features(posts)
    
    def _create_simple_features(self, posts: List[str]) -> torch.Tensor:
        """Fallback simple feature creation"""
        post_features = []
        
        for post in posts:
            features = torch.zeros(768)
            
            # Basic content features
            features[0] = len(post) / 100.0  # Normalized length
            features[1] = len(self.extract_words(post)) / 50.0  # Normalized word count
            features[2] = len(self.extract_emojis(post)) / 10.0  # Normalized emoji count
            
            # Word presence features
            words = self.extract_words(post)
            for word in words[:100]:  # Use first 100 words
                if word in self.word_vocab:
                    word_id = self.word_vocab[word]
                    if word_id < 765:  # Leave space for other features
                        features[3 + word_id] = 1.0
            
            post_features.append(features)
        
        return torch.stack(post_features)
    
    def create_word_features(self, words: List[str]) -> torch.Tensor:
        """Create features for words using TF-IDF scores"""
        word_features = []
        
        for word in words:
            features = torch.zeros(768)
            
            # Basic word features
            features[0] = len(word) / 10.0  # Normalized length
            features[1] = self.word_idf_scores.get(word, 0.0)  # IDF score
            
            # Character-level features
            char_hash = hash(word) % 766
            features[2 + char_hash] = 1.0
            
            word_features.append(features)
        
        return torch.stack(word_features) if word_features else torch.zeros(0, 768)
    
    def create_emoji_features(self, emojis: List[str]) -> torch.Tensor:
        """Create features for emojis using learned representations"""
        emoji_features = []
        
        for emoji in emojis:
            # Use random features as in original implementation
            # but make them consistent for the same emoji
            np.random.seed(hash(emoji) % (2**32))
            features = torch.tensor(np.random.randn(768), dtype=torch.float32)
            emoji_features.append(features)
        
        return torch.stack(emoji_features) if emoji_features else torch.zeros(0, 768)
    
    def build_vocabulary(self, posts: List[str]):
        """Build vocabulary mappings for new posts"""
        # For new posts, just map them to indices
        for i, post in enumerate(posts):
            if post not in self.post_vocab:
                self.post_vocab[post] = len(self.post_vocab)
    
    def create_heterogeneous_graph(self, posts: List[str]) -> dgl.DGLGraph:
        """
        Create heterogeneous graph with proper features matching training data
        """
        # Build post vocabulary
        self.build_vocabulary(posts)
        
        # Get all unique words and emojis from posts
        all_words = set()
        all_emojis = set()
        
        for post in posts:
            words = self.extract_words(post)
            emojis = self.extract_emojis(post)
            all_words.update(words)
            all_emojis.update(emojis)
        
        # Filter to known vocabulary
        known_words = [w for w in all_words if w in self.word_vocab]
        known_emojis = [e for e in all_emojis if e in self.emoji_vocab]
        
        logger.info(f"Creating graph with {len(posts)} posts, {len(known_words)} words, {len(known_emojis)} emojis")
        
        # Create node features using proper methods
        post_features = self.create_tfidf_features(posts)
        word_features = self.create_word_features(known_words)
        emoji_features = self.create_emoji_features(known_emojis)
        
        # Create word and emoji mappings for this graph
        word_to_idx = {word: i for i, word in enumerate(known_words)}
        emoji_to_idx = {emoji: i for i, emoji in enumerate(known_emojis)}
        
        # Collect edges
        post_word_edges = []
        post_emoji_edges = []
        word_emoji_edges = []
        
        for post_idx, post in enumerate(posts):
            words = self.extract_words(post)
            emojis = self.extract_emojis(post)
            
            # Post-word edges
            for word in words:
                if word in word_to_idx:
                    word_idx = word_to_idx[word]
                    post_word_edges.append((post_idx, word_idx))
            
            # Post-emoji edges  
            for emoji in emojis:
                if emoji in emoji_to_idx:
                    emoji_idx = emoji_to_idx[emoji]
                    post_emoji_edges.append((post_idx, emoji_idx))
            
            # Word-emoji co-occurrence edges
            for word in words:
                if word in word_to_idx:
                    word_idx = word_to_idx[word]
                    for emoji in emojis:
                        if emoji in emoji_to_idx:
                            emoji_idx = emoji_to_idx[emoji]
                            word_emoji_edges.append((word_idx, emoji_idx))
        
        # Create heterogeneous graph
        graph_data = {}
        
        # Add edges with correct relation names matching training data
        if post_word_edges:
            post_ids, word_ids = zip(*post_word_edges)
            graph_data[('post', 'hasw', 'word')] = (torch.tensor(post_ids), torch.tensor(word_ids))
            graph_data[('word', 'win', 'post')] = (torch.tensor(word_ids), torch.tensor(post_ids))
        
        if post_emoji_edges:
            post_ids, emoji_ids = zip(*post_emoji_edges)
            graph_data[('post', 'hase', 'emoji')] = (torch.tensor(post_ids), torch.tensor(emoji_ids))
            graph_data[('emoji', 'ein', 'post')] = (torch.tensor(emoji_ids), torch.tensor(post_ids))
        
        if word_emoji_edges:
            word_ids, emoji_ids = zip(*word_emoji_edges)
            graph_data[('word', 'withe', 'emoji')] = (torch.tensor(word_ids), torch.tensor(emoji_ids))
            graph_data[('emoji', 'by', 'word')] = (torch.tensor(emoji_ids), torch.tensor(word_ids))
        
        # Create graph
        if graph_data:
            g = dgl.heterograph(graph_data)
        else:
            # Create empty graph with correct node types
            g = dgl.heterograph({
                ('post', 'dummy', 'post'): ([], []),
                ('word', 'dummy', 'word'): ([], []),
                ('emoji', 'dummy', 'emoji'): ([], [])
            })
        
        # Set node features
        if len(posts) > 0:
            g.nodes['post'].data['feat'] = post_features
        if len(known_words) > 0:
            g.nodes['word'].data['feat'] = word_features
        if len(known_emojis) > 0:
            g.nodes['emoji'].data['feat'] = emoji_features
        
        logger.info(f"Graph created successfully: {g}")
        return g
    
    def get_post_embedding_from_pretrained_model(self, post: str, model) -> torch.Tensor:
        """
        Get embedding for a post using the pre-trained model
        Uses proper feature representation matching training setup
        """
        try:
            # Create graph with proper features
            graph = self.create_heterogeneous_graph([post])
            
            # Add seed information (required by the model)
            # Mark all nodes as seed nodes for this inference
            for ntype in graph.ntypes:
                num_nodes = graph.num_nodes(ntype)
                if num_nodes > 0:
                    graph.nodes[ntype].data['seed'] = torch.zeros(num_nodes, dtype=torch.long)
            
            # Move graph to the same device as model
            device = next(model.parameters()).device
            graph = graph.to(device)
            
            # Use the pre-trained model to get embeddings
            with torch.no_grad():
                # Try different edge types based on what's available in the graph
                available_etypes = graph.canonical_etypes
                
                # Priority order for edge types (based on training data)
                preferred_etypes = [
                    ('emoji', 'ein', 'post'),
                    ('post', 'hase', 'emoji'), 
                    ('post', 'hasw', 'word'),
                    ('word', 'withe', 'emoji')
                ]
                
                embedding = None
                for etype in preferred_etypes:
                    if etype in available_etypes and graph.num_edges(etype) > 0:
                        try:
                            # Call model with proper edge type
                            embedding = model(graph, etype)
                            # Get the first post's embedding (index 0)
                            if embedding.dim() > 1:
                                embedding = embedding[0]  # First post
                            break
                        except Exception as e:
                            logger.debug(f"Failed with edge type {etype}: {e}")
                            continue
                
                # If no edge-based embedding worked, use post features directly
                if embedding is None:
                    if graph.num_nodes('post') > 0:
                        embedding = graph.nodes['post'].data['feat'][0]
                    else:
                        # Fallback to zero embedding with correct dimension
                        embedding = torch.zeros(768, device=device)
                
                return embedding.squeeze().cpu()
                
        except Exception as e:
            logger.error(f"Error getting embedding with model: {e}")
            # Fallback to TF-IDF features
            return self.create_tfidf_features([post])[0]
    
    def find_similar_emojis_using_model(self, post_content: str, model, top_k: int = 5) -> List[str]:
        """
        Find emojis semantically similar to post using the pre-trained model
        Uses actual emoji embeddings from the pre-trained model
        """
        try:
            logger.info(f"ðŸŽ­ Finding similar emojis using pre-trained model for: {post_content[:50]}...")
            
            # Get candidate emojis from our vocabulary (excluding ones already in post)
            current_emojis = set(self.extract_emojis(post_content))
            candidate_emojis = [emoji for emoji in self.emoji_vocab.keys() if emoji not in current_emojis]
            
            if not candidate_emojis:
                logger.warning("No candidate emojis available")
                return []
            
            # Use the model's emoji embeddings directly
            # Create a graph with post + all candidate emojis to get their embeddings
            test_posts = [post_content] + [f"emoji {emoji}" for emoji in candidate_emojis[:50]]  # Limit for performance
            
            try:
                # Create graph with all posts and emojis
                graph = self.create_heterogeneous_graph(test_posts)
                
                # Add seed information
                for ntype in graph.ntypes:
                    num_nodes = graph.num_nodes(ntype)
                    if num_nodes > 0:
                        graph.nodes[ntype].data['seed'] = torch.zeros(num_nodes, dtype=torch.long)
                
                # Get embeddings using the model
                device = next(model.parameters()).device
                graph = graph.to(device)
                
                with torch.no_grad():
                    # Try to get emoji embeddings from the model
                    if ('emoji', 'ein', 'post') in graph.canonical_etypes and graph.num_edges(('emoji', 'ein', 'post')) > 0:
                        embeddings = model(graph, ('emoji', 'ein', 'post'))
                        
                        # Get emoji node embeddings
                        if graph.num_nodes('emoji') > 0:
                            emoji_embeddings = graph.nodes['emoji'].data['feat']  # Use original features
                            post_embedding = graph.nodes['post'].data['feat'][0]  # First post
                            
                            # Calculate similarities between post and emojis
                            emoji_similarities = []
                            emoji_list = list(self.emoji_vocab.keys())[:graph.num_nodes('emoji')]
                            
                            for i, emoji in enumerate(emoji_list):
                                if emoji not in current_emojis and i < len(emoji_embeddings):
                                    similarity = torch.cosine_similarity(
                                        post_embedding.unsqueeze(0),
                                        emoji_embeddings[i].unsqueeze(0)
                                    ).item()
                                    emoji_similarities.append((emoji, similarity))
                            
                            # Sort by similarity and return top k
                            emoji_similarities.sort(key=lambda x: x[1], reverse=True)
                            top_emojis = [emoji for emoji, _ in emoji_similarities[:top_k]]
                            
                            if top_emojis:
                                logger.info(f"âœ… Found {len(top_emojis)} similar emojis using model: {top_emojis}")
                                return top_emojis
                
            except Exception as e:
                logger.debug(f"Graph-based emoji similarity failed: {e}")
            
            # Fallback: Use semantic similarity based on emoji frequency and post content
            logger.info("Using frequency-based emoji selection as fallback")
            
            # Extract key words from post content
            words = self.extract_words(post_content)
            word_set = set(words)
            
            # Score emojis based on co-occurrence with words in the vocabulary
            emoji_scores = []
            for emoji in candidate_emojis[:100]:  # Limit for performance
                score = 0
                
                # Basic frequency score (more frequent = higher base score)
                if emoji in self.emoji_vocab:
                    frequency = self.emoji_vocab[emoji]
                    score += min(frequency / 1000, 1.0)  # Normalize frequency
                
                # Semantic bonus: if emoji appears with similar words in our data
                # This is a simple heuristic - in practice, you'd use the actual co-occurrence data
                emoji_words = self.extract_words(emoji)  # Extract any words from emoji description
                word_overlap = len(word_set.intersection(set(emoji_words)))
                score += word_overlap * 0.1
                
                emoji_scores.append((emoji, score))
            
            # Sort by score and return top k
            emoji_scores.sort(key=lambda x: x[1], reverse=True)
            top_emojis = [emoji for emoji, _ in emoji_scores[:top_k]]
            
            logger.info(f"âœ… Selected {len(top_emojis)} emojis using semantic fallback: {top_emojis}")
            return top_emojis
            
        except Exception as e:
            logger.error(f"Error finding similar emojis: {e}")
            return self._fallback_emoji_suggestions(post_content, top_k)
    
    def _fallback_emoji_suggestions(self, post_content: str, top_k: int = 5) -> List[str]:
        """Fallback emoji suggestions"""
        # Use most common emojis from our vocabulary
        current_emojis = set(self.extract_emojis(post_content))
        common_emojis = list(self.emoji_vocab.keys())[:50]  # Top 50 most common
        suggestions = [emoji for emoji in common_emojis if emoji not in current_emojis]
        return suggestions[:top_k]
    
    def get_vocabulary_stats(self) -> Dict[str, int]:
        """Get vocabulary statistics"""
        return {
            'words': len(self.word_vocab),
            'emojis': len(self.emoji_vocab),
            'posts': len(self.post_vocab)
        } 